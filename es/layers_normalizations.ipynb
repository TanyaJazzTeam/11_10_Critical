{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFPyjGqMQ82Q"
      },
      "source": [
        "##### Copyright 2020 Los autores de TensorFlow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aNZ7aEDyQIYU"
      },
      "outputs": [

      ],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMOmzhPEQh7b"
      },
      "source": [
        "# Normalizaciones\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/addons/tutorials/layers_normalizations\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/addons/blob/master/docs/tutorials/layers_normalizations.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/addons/blob/master/docs/tutorials/layers_normalizations.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fuente en GitHub</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/addons/docs/tutorials/layers_normalizations.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar libreta</a></td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cthm5dovQMJl"
      },
      "source": [
        "## Visión general\n",
        "\n",
        "Este cuaderno brinda una breve introducción a las [capas de normalización](https://github.com/tensorflow/addons/blob/master/tensorflow_addons/layers/normalizations.py) de TensorFlow. Las capas admitidas actualmente son:\n",
        "\n",
        "- **Normalización de grupos** (complementos de TensorFlow)\n",
        "- **Normalización de instancias** (complementos de TensorFlow)\n",
        "- **Normalización de capas** (TensorFlow Core)\n",
        "\n",
        "La idea básica detrás de estas capas es normalizar la salida de una capa de activación para mejorar la convergencia durante el entrenamiento. A diferencia de la normalización por [lotes](https://keras.io/layers/normalization/) , estas normalizaciones no funcionan en lotes, sino que normalizan las activaciones de una sola muestra, lo que las hace adecuadas también para redes neuronales recurrentes.\n",
        "\n",
        "Por lo general, la normalización se realiza calculando la media y la desviación estándar de un subgrupo en su tensor de entrada. También es posible aplicar una escala y un factor de compensación a esto.\n",
        "\n",
        "$y_{i} = \\frac{\\gamma ( x_{i} - \\mu )}{\\sigma }+ \\beta$\n",
        "\n",
        "$ y$ : Salida\n",
        "\n",
        "$x$ : Entrada\n",
        "\n",
        "$\\gamma$ : Factor de escala\n",
        "\n",
        "$\\mu$: media\n",
        "\n",
        "$\\sigma$: desviación estándar\n",
        "\n",
        "$\\beta$: factor de compensación\n",
        "\n",
        "La siguiente imagen demuestra la diferencia entre estas técnicas. Cada subparcela muestra un tensor de entrada, con N como el eje del lote, C como el eje del canal y (H, W) como los ejes espaciales (alto y ancho de una imagen, por ejemplo). Los píxeles en azul están normalizados por la misma media y varianza, calculados agregando los valores de estos píxeles.\n",
        "\n",
        "![](https://github.com/shaohua0116/Group-Normalization-Tensorflow/raw/master/figure/gn.png)\n",
        "\n",
        "Fuente: (https://arxiv.org/pdf/1803.08494.pdf)\n",
        "\n",
        "Los pesos gamma y beta se pueden entrenar en todas las capas de normalización para compensar la posible pérdida de capacidad de representación. Puede activar estos factores configurando el `center` o la bandera de la `scale` en `True` . Por supuesto, puede usar `initializers` , `constraints` y `regularizer` para `beta` y `gamma` para ajustar estos valores durante el proceso de entrenamiento. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2XlcXf5WBHb"
      },
      "source": [
        "## Configuración"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2XlcXf5WBHi"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTlbneoEUKrD"
      },
      "source": [
        "### Instalar Tensorflow 2.0 y Tensorflow-Addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZQGY_ALnirQ"
      },
      "outputs": [

      ],
      "source": [
        "!pip install -U tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aGgPZG_WBHg"
      },
      "outputs": [

      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u82Gz_gOUPDZ"
      },
      "source": [
        "### Preparación del conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wso9oidUZZQ"
      },
      "outputs": [

      ],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTQH56j89POZ"
      },
      "source": [
        "## Tutorial de normalización de grupos\n",
        "\n",
        "### Introducción\n",
        "\n",
        "La normalización de grupo (GN) divide los canales de sus entradas en subgrupos más pequeños y normaliza estos valores en función de su media y varianza. Dado que GN trabaja en un solo ejemplo, esta técnica es independiente del tamaño del lote.\n",
        "\n",
        "GN puntuó experimentalmente cerca de la normalización por lotes en tareas de clasificación de imágenes. Puede ser beneficioso usar GN en lugar de la Normalización por lotes en caso de que su tamaño de lote general sea bajo, lo que conduciría a un mal rendimiento de la normalización por lotes.\n",
        "\n",
        "### Ejemplo\n",
        "\n",
        "Dividir 10 canales después de una capa Conv2D en 5 subgrupos en una configuración estándar de \"canales últimos\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIGjLwYWAm0v"
      },
      "outputs": [

      ],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  # Reshape into \"channels last\" setup.\n",
        "  tf.keras.layers.Reshape((28,28,1), input_shape=(28,28)),\n",
        "  tf.keras.layers.Conv2D(filters=10, kernel_size=(3,3),data_format=\"channels_last\"),\n",
        "  # Groupnorm Layer\n",
        "  tfa.layers.GroupNormalization(groups=5, axis=3),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMwUfJUib3ka"
      },
      "source": [
        "## Tutorial de normalización de instancias\n",
        "\n",
        "### Introducción\n",
        "\n",
        "La normalización de instancias es un caso especial de normalización de grupos en el que el tamaño del grupo es del mismo tamaño que el tamaño del canal (o el tamaño del eje).\n",
        "\n",
        "Los resultados experimentales muestran que la normalización de instancias funciona bien en la transferencia de estilo cuando se reemplaza la normalización por lotes. Recientemente, la normalización de instancias también se ha utilizado como reemplazo de la normalización por lotes en las GAN.\n",
        "\n",
        "###Ejemplo Aplicar InstanceNormalization después de una capa Conv2D y usar una escala inicializada uniformada y un factor de compensación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sLVv-C8f6Kf"
      },
      "outputs": [

      ],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  # Reshape into \"channels last\" setup.\n",
        "  tf.keras.layers.Reshape((28,28,1), input_shape=(28,28)),\n",
        "  tf.keras.layers.Conv2D(filters=10, kernel_size=(3,3),data_format=\"channels_last\"),\n",
        "  # LayerNorm Layer\n",
        "  tfa.layers.InstanceNormalization(axis=3, \n",
        "                                   center=True, \n",
        "                                   scale=True,\n",
        "                                   beta_initializer=\"random_uniform\",\n",
        "                                   gamma_initializer=\"random_uniform\"),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYdnEocRUCll"
      },
      "source": [
        "## Tutorial de normalización de capas\n",
        "\n",
        "### Introducción\n",
        "\n",
        "La normalización de capas es un caso especial de normalización de grupo donde el tamaño del grupo es 1. La media y la desviación estándar se calculan a partir de todas las activaciones de una sola muestra.\n",
        "\n",
        "Los resultados experimentales muestran que la normalización de capas es adecuada para las redes neuronales recurrentes, ya que funciona de forma independiente en el tamaño del lote.\n",
        "\n",
        "### Ejemplo\n",
        "\n",
        "Aplicar la normalización de capa después de una capa Conv2D y usar un factor de escala y desplazamiento. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh-Pp_e5UB54"
      },
      "outputs": [

      ],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  # Reshape into \"channels last\" setup.\n",
        "  tf.keras.layers.Reshape((28,28,1), input_shape=(28,28)),\n",
        "  tf.keras.layers.Conv2D(filters=10, kernel_size=(3,3),data_format=\"channels_last\"),\n",
        "  # LayerNorm Layer\n",
        "  tf.keras.layers.LayerNormalization(axis=3 , center=True , scale=True),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shvGfnB0WpQQ"
      },
      "source": [
        "## Literatura\n",
        "\n",
        "[Norma de capa](https://arxiv.org/pdf/1607.06450.pdf)\n",
        "\n",
        "[Norma de instancia](https://arxiv.org/pdf/1607.08022.pdf)\n",
        "\n",
        "[Norma de grupo](https://arxiv.org/pdf/1803.08494.pdf)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [

      ],
      "name": "layers_normalizations.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
